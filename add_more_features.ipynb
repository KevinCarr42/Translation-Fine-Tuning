{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:05:41.661177Z",
     "start_time": "2025-08-15T11:05:40.577498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "from spellchecker import SpellChecker\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from create_jsonl import save_jsonl\n",
    "\n",
    "# pandas formatting\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# the same dataset after cleaning\n",
    "df_features = pd.read_pickle(\"df_with_features.pickle\")\n",
    "\n",
    "percentiles = [0.00135, 0.02275, 0.1587, 0.8413, 0.97725, 0.99865]\n",
    "columns = ['50%', '0.1%', '2.3%', '15.9%', '84.1%', '97.7%', '99.9%']"
   ],
   "id": "c002d1ba002a9b01",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# New Feature - number of 1-character words",
   "id": "d15ef3d7d85f2627"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "actual_one_char_words_fr = ['À', 'A', 'L', 'D', 'N', 'Y', 'M', 'S', 'T', 'à', 'a', 'l', 'd', 'n', 'y', 'm', 's', 't']\n",
    "df_features['one_char_words_fr'] = df_features['fr'].apply(lambda s: sum(len(w) == 1 for w in s.split() if w not in actual_one_char_words_fr))\n",
    "\n",
    "actual_one_char_words_en = ['A', 'I', 'O', 'a', 'o']\n",
    "df_features['one_char_words_en'] = df_features['en'].apply(lambda s: sum(len(w) == 1 for w in s.split() if w not in actual_one_char_words_en))"
   ],
   "id": "de419cccd3d67f4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cleaning Single Letter Words and Missing Apostrophes (fixing bad OCR)",
   "id": "4650e85b45c2ca52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:05:58.533012Z",
     "start_time": "2025-08-15T11:05:58.462943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# make a backup to check before replacements to check results\n",
    "df_features_backup1 = df_features.copy()\n",
    "df_features_backup1 = df_features_backup1[['fr']]\n",
    "df_features_backup1.columns = ['fr_before']"
   ],
   "id": "20eb458d73faf490",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:06:00.001995Z",
     "start_time": "2025-08-15T11:05:58.610389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "always_have_apostrophe = ['L', 'D', 'N', 'M', 'S', 'T', 'l', 'd', 'n', 'm', 's', 't']\n",
    "missing_apostrophe_patterns = [\" \" + x + \" \" for x in always_have_apostrophe]\n",
    "replacement_patterns = [\" \" + x + \"'\" for x in always_have_apostrophe]\n",
    "\n",
    "n_with_missing = df_features.loc[\n",
    "    df_features['fr'].str.contains('|'.join(missing_apostrophe_patterns), na=False, case=False),\n",
    "].shape[0]\n",
    "n_total = df_features.shape[0]\n",
    "print(f\"{n_with_missing} out of {n_total} sentences are missing apostrophes ({n_with_missing / n_total:.0%})\")"
   ],
   "id": "48d255389ee07061",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420248 out of 778951 sentences are missing apostrophes (54%)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:06:01.111632Z",
     "start_time": "2025-08-15T11:06:00.642217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normal_apostrophe, curved_apostrophe = \"'\", \"’\"\n",
    "\n",
    "print(f\"Normal apostrophe ({normal_apostrophe}): {df_features['fr'].str.count(normal_apostrophe).sum()}\")\n",
    "print(f\"Curved apostrophe ({curved_apostrophe}): {df_features['fr'].str.count(curved_apostrophe).sum()}\")"
   ],
   "id": "b8e5f8f0f7c6ae5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal apostrophe ('): 200432\n",
      "Curved apostrophe (’): 0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:06:10.948966Z",
     "start_time": "2025-08-15T11:06:02.073861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# clean up these OCR apostrophe issues\n",
    "df_features['fr'] = df_features['fr'].replace(\n",
    "    dict(zip(missing_apostrophe_patterns, replacement_patterns)), \n",
    "    regex=True\n",
    ")\n",
    "\n",
    "n_with_missing = df_features.loc[\n",
    "    df_features['fr'].str.contains('|'.join(missing_apostrophe_patterns), na=False, case=False),\n",
    "].shape[0]\n",
    "n_total = df_features.shape[0]\n",
    "print(f\"after cleaning, {n_with_missing} out of {n_total} sentences are missing apostrophes ({n_with_missing / n_total:.0%})\")"
   ],
   "id": "d714f55174741a99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after cleaning, 0 out of 778951 sentences are missing apostrophes (0%)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:28:19.775608Z",
     "start_time": "2025-08-15T11:28:12.692326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# how much difference?\n",
    "\n",
    "before = int(df_features_backup1['fr_before'].apply(lambda s: sum(len(w) == 1 for w in s.split() if w not in actual_one_char_words_fr)).sum())\n",
    "after = int(df_features['fr'].apply(lambda s: sum(len(w) == 1 for w in s.split() if w not in actual_one_char_words_fr)).sum())\n",
    "difference = before - after\n",
    "\n",
    "before, after, difference"
   ],
   "id": "c10745b71b58eed2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291800, 288972, 2828)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# check for missing non-english symbols",
   "id": "e3f6acb414bddd84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:06:30.049609Z",
     "start_time": "2025-08-15T11:06:19.346512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def has_non_english_chars(word):\n",
    "    return bool(re.search(r'[^\\x00-\\x7F]', word))\n",
    "\n",
    "def remove_accents(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "french_words_with_accents = []\n",
    "for sentence in df_features['fr'].to_list():\n",
    "    for word in sentence.split():\n",
    "        clean_word = word.replace('(', '').replace(')', '')\n",
    "        if clean_word.isalpha() and has_non_english_chars(clean_word):\n",
    "            french_words_with_accents.append(clean_word.lower())\n",
    "\n",
    "word_counts = Counter(french_words_with_accents)\n",
    "\n",
    "accent_mapping = pd.DataFrame([\n",
    "    {\n",
    "        'anglicised': remove_accents(word),\n",
    "        'accented': word,\n",
    "        'count': count\n",
    "    }\n",
    "    for word, count in word_counts.items()\n",
    "]).sort_values('count', ascending=False).reset_index(drop=True)"
   ],
   "id": "76c18dfe74490edf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:06:30.796711Z",
     "start_time": "2025-08-15T11:06:30.677401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check accent_mapping for duplicates \n",
    "#  create list of duplicates (to classify as potential quality issues)\n",
    "potential_accent_issues_ambiguous = accent_mapping.loc[accent_mapping.duplicated('anglicised', keep=\"first\"), 'anglicised'].to_list()\n",
    "#  split into non-duplicates (to clean)\n",
    "accent_mapping = accent_mapping.drop_duplicates('anglicised', keep=False)\n",
    "\n",
    "# check for real words in mispelled list\n",
    "spell = SpellChecker(language='fr')\n",
    "#  add all anglicised words that are real french words to another different potentially bad word list\n",
    "potential_accent_issues_real_words = accent_mapping.loc[accent_mapping['anglicised'].isin(spell), 'anglicised'].to_list()\n",
    "#  remove all anglicised words that are real french words \n",
    "accent_mapping = accent_mapping[~accent_mapping['anglicised'].isin(spell)]\n",
    "\n",
    "# NOTE: could there be multiple correct fixes? not in the corpus of data, we've already checked for duplicates, so should be low chances\n",
    "\n",
    "# take the top 1000 most common words that could be cleaned\n",
    "#  add the rest to the potentially bad words\n",
    "potential_accent_issues_uncommon = accent_mapping.tail(accent_mapping.shape[0]-1000).anglicised.to_list()\n",
    "# create dict from remaining words for cleaning\n",
    "accent_mapping = accent_mapping.head(1000)\n",
    "replacement_dict = accent_mapping.set_index('anglicised')['accented'].to_dict()"
   ],
   "id": "1671e02279213849",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:06:31.406060Z",
     "start_time": "2025-08-15T11:06:31.260574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# backup again before replacing\n",
    "\n",
    "df_features_backup2 = df_features.copy()\n",
    "df_features_backup2 = df_features_backup2[['fr']]\n",
    "df_features_backup2.columns = ['fr_before']"
   ],
   "id": "c59e56a7f3327376",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:09:36.306579Z",
     "start_time": "2025-08-15T11:06:32.443649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_replacement_regex(replacement_map):\n",
    "    pattern = r'\\b(' + '|'.join([re.escape(k) for k in replacement_map.keys()]) + r')\\b'\n",
    "    \n",
    "    def replace_func(match):\n",
    "        matched_word = match.group(1)\n",
    "        return replacement_map.get(matched_word, matched_word)\n",
    "    \n",
    "    return pattern, replace_func\n",
    "\n",
    "pattern, replace_func = create_replacement_regex(replacement_dict)\n",
    "df_features['fr'] = df_features['fr'].str.replace(pattern, replace_func, regex=True)"
   ],
   "id": "145566da9b85a2ee",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:09:36.911019Z",
     "start_time": "2025-08-15T11:09:36.417089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# comparisons\n",
    "\n",
    "df_features_compare1 = pd.concat([\n",
    "    df_features, df_features_backup1\n",
    "], axis=1)[['fr', 'fr_before']]\n",
    "df_features_compare1 = df_features_compare1[df_features_compare1.fr != df_features_compare1.fr_before]\n",
    "\n",
    "df_features_compare2 = pd.concat([\n",
    "    df_features, df_features_backup2\n",
    "], axis=1)[['fr', 'fr_before']]\n",
    "\n",
    "df_features_compare2 = df_features_compare2[df_features_compare2.fr != df_features_compare2.fr_before]\n",
    "\n",
    "print(df_features_compare1.shape[0], 'corrections of single letter words')\n",
    "print(df_features_compare2.shape[0], 'corrections of mis-accented words')"
   ],
   "id": "47005072309f8488",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:17:57.543142Z",
     "start_time": "2025-08-15T11:17:57.524132Z"
    }
   },
   "cell_type": "code",
   "source": "df_features_compare1.sample().T",
   "id": "c285971a6cc9b083",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                         36852\n",
       "fr         La discussion se concentre sur les lieux de frai et les contradictions apparentes entre le degré...\n",
       "fr_before  La discussion se concentre sur les lieux de frai et les contradictions apparentes entre le degré..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>36852</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>La discussion se concentre sur les lieux de frai et les contradictions apparentes entre le degré...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_before</th>\n",
       "      <td>La discussion se concentre sur les lieux de frai et les contradictions apparentes entre le degré...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:17:53.683676Z",
     "start_time": "2025-08-15T11:17:53.677497Z"
    }
   },
   "cell_type": "code",
   "source": "df_features_compare2.sample().T",
   "id": "ea9be3f89050fea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                        147645\n",
       "fr         Les plus récents releves de recherche de printemps canadiens et américains indiquent une diminut...\n",
       "fr_before  Les plus recents releves de recherche de printemps canadiens et americains indiquent une diminut..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>147645</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>Les plus récents releves de recherche de printemps canadiens et américains indiquent une diminut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_before</th>\n",
       "      <td>Les plus recents releves de recherche de printemps canadiens et americains indiquent une diminut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# create troublesome accented words exclusions",
   "id": "316b1209e2ee6a31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:39:22.619260Z",
     "start_time": "2025-08-15T11:39:22.609356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# potential_accent_issues_ambiguous \n",
    "#  this list is useless, there are so many words that could be accented differently, and many are very common\n",
    "df_features['fr'].head(10).apply(lambda s: sum(w in potential_accent_issues_ambiguous for w in s.split()))"
   ],
   "id": "42286afce05d993",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23\n",
       "1    17\n",
       "2    13\n",
       "3    13\n",
       "4     7\n",
       "5     4\n",
       "6    10\n",
       "7     7\n",
       "8     5\n",
       "9     6\n",
       "Name: fr, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:40:21.808619Z",
     "start_time": "2025-08-15T11:40:21.791439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# potential_accent_issues_real_words\n",
    "#  this list is also pretty useless, too many matches\n",
    "df_features['fr'].head(10).apply(lambda s: sum(w in potential_accent_issues_real_words for w in s.split()))"
   ],
   "id": "f25d2aaf2cf407ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    3\n",
       "2    6\n",
       "3    8\n",
       "4    2\n",
       "5    3\n",
       "6    2\n",
       "7    1\n",
       "8    0\n",
       "9    1\n",
       "Name: fr, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:40:54.764237Z",
     "start_time": "2025-08-15T11:40:54.735275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# potential_accent_issues_uncommon \n",
    "#  these look useful - the anglicised fr words with no potential duplicate, that weren't replaced above\n",
    "df_features['fr'].head(10).apply(lambda s: sum(w in potential_accent_issues_uncommon for w in s.split()))"
   ],
   "id": "5fbe4c6cde798de3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    0\n",
       "8    0\n",
       "9    0\n",
       "Name: fr, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T11:51:19.600659Z",
     "start_time": "2025-08-15T11:42:55.163931Z"
    }
   },
   "cell_type": "code",
   "source": "df_features['potential_fr_accent_issues'] = df_features['fr'].apply(lambda s: sum(w in potential_accent_issues_uncommon for w in s.split()))",
   "id": "8f70bc9e0941deeb",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpotential_fr_accent_issues\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf_features\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpotential_accent_issues_uncommon\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Repositories\\dm_apps_root\\jupyter_venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4800\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Repositories\\dm_apps_root\\jupyter_venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Repositories\\dm_apps_root\\jupyter_venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[0;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\Documents\\Repositories\\dm_apps_root\\jupyter_venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Repositories\\dm_apps_root\\jupyter_venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[55], line 1\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(s)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpotential_fr_accent_issues\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfr\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m s: \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpotential_accent_issues_uncommon\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[1;32mIn[55], line 1\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpotential_fr_accent_issues\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfr\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m s: \u001B[38;5;28msum\u001B[39m(w \u001B[38;5;129;01min\u001B[39;00m potential_accent_issues_uncommon \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m s\u001B[38;5;241m.\u001B[39msplit()))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T12:01:50.033399Z",
     "start_time": "2025-08-15T11:52:34.524050Z"
    }
   },
   "cell_type": "code",
   "source": "df_features['potential_fr_accent_issues'] = df_features['fr'].str.split().apply(lambda lst: sum(w in set(potential_accent_issues_uncommon) for w in lst))\n",
   "id": "a2db8fe2b6888d07",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[56], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpotential_fr_accent_issues\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf_features\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlst\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mset\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpotential_accent_issues_uncommon\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlst\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Repositories\\dm_apps_root\\jupyter_venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4800\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Repositories\\dm_apps_root\\jupyter_venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Repositories\\dm_apps_root\\jupyter_venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[0;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\Documents\\Repositories\\dm_apps_root\\jupyter_venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Repositories\\dm_apps_root\\jupyter_venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[56], line 1\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(lst)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpotential_fr_accent_issues\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfr\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39msplit()\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m lst: \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mset\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpotential_accent_issues_uncommon\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlst\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[1;32mIn[56], line 1\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpotential_fr_accent_issues\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfr\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39msplit()\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m lst: \u001B[38;5;28msum\u001B[39m(w \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mset\u001B[39m(potential_accent_issues_uncommon) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m lst))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T12:46:19.128389Z",
     "start_time": "2025-08-15T12:02:06.402583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NOTE: very slow, takes about an hour to calculate this\n",
    "pattern = r'\\b(?:' + '|'.join(map(re.escape, potential_accent_issues_uncommon)) + r')\\b'\n",
    "df_features['potential_fr_accent_issues'] = df_features['fr'].str.count(pattern)"
   ],
   "id": "19554a56c6ba85ea",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T12:53:13.434047Z",
     "start_time": "2025-08-15T12:53:13.404053Z"
    }
   },
   "cell_type": "code",
   "source": "pd.DataFrame(df_features['potential_fr_accent_issues'].describe(percentiles=[0.9, 0.99, 0.999])).T",
   "id": "8f8df70ebcaf26d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                               count  mean  std  min  50%  90%  99%  99.9%  \\\n",
       "potential_fr_accent_issues 778951.00  0.03 0.19 0.00 0.00 0.00 1.00   2.00   \n",
       "\n",
       "                             max  \n",
       "potential_fr_accent_issues 18.00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>50%</th>\n",
       "      <th>90%</th>\n",
       "      <th>99%</th>\n",
       "      <th>99.9%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>potential_fr_accent_issues</th>\n",
       "      <td>778951.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# save the file",
   "id": "c99f4a7d10f34af4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T12:50:28.597850Z",
     "start_time": "2025-08-15T12:50:27.469670Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 58,
   "source": "df_features.to_pickle('df_with_more_features.pickle')",
   "id": "ed97706361a04972"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2a4ef216250e337d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
